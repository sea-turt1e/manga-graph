#!/usr/bin/env python
"""Import MAL API JSON data into Neo4j as Work nodes.

This script reads JSON files generated by fetch_manga_from_mal_api.py or
fetch_anime_from_mal_api.py and creates Work nodes in Neo4j.

Unlike the CSV importer, this script also creates:
- Author nodes and CREATED_BY relationships
- Magazine nodes and PUBLISHED_IN relationships  
- Studio nodes and PRODUCED_BY relationships (for anime)
- Related work relationships (RELATED_TO)

Usage:
    # Import manga data
    python scripts/data_import/import_mal_api_to_neo4j.py --type manga
    
    # Import anime data
    python scripts/data_import/import_mal_api_to_neo4j.py --type anime
    
    # Import both
    python scripts/data_import/import_mal_api_to_neo4j.py --type both
    
    # Dry run to see what would be imported
    python scripts/data_import/import_mal_api_to_neo4j.py --type manga --dry-run
    
    # Reset (delete existing nodes before import)
    python scripts/data_import/import_mal_api_to_neo4j.py --type manga --reset
"""

from __future__ import annotations

import argparse
import json
import math
import os
import sys
from dataclasses import dataclass
from pathlib import Path
from typing import Any, Dict, Iterable, Iterator, List, Optional, Set

from neo4j import Driver, GraphDatabase
from tqdm.auto import tqdm

# Ensure the project root is on sys.path
PROJECT_ROOT = Path(__file__).resolve().parents[2]
if str(PROJECT_ROOT) not in sys.path:
    sys.path.insert(0, str(PROJECT_ROOT))

from config import env  # noqa: F401

DEFAULT_PUBLISHER_MAPPING = Path(
    "data/myanimelist/myanimelist-scraped-data-2025-July/publisher_magazine_mapping.json"
)


@dataclass
class ImportConfig:
    input_dir: Path
    import_type: str  # "manga", "anime", or "both"
    uri: str
    user: str
    password: str
    batch_size: int = 500
    limit: Optional[int] = None
    dry_run: bool = False
    reset: bool = False
    create_relationships: bool = True
    publisher_mapping_path: Optional[Path] = None


def parse_args(argv: Optional[List[str]] = None) -> ImportConfig:
    parser = argparse.ArgumentParser(
        description="Import MAL API JSON data into Neo4j",
        formatter_class=argparse.ArgumentDefaultsHelpFormatter,
    )
    parser.add_argument(
        "--input-dir",
        type=Path,
        default=Path("data/mal_api"),
        help="Directory containing JSON files from fetch scripts",
    )
    parser.add_argument(
        "--type",
        type=str,
        default="both",
        choices=["manga", "anime", "both"],
        help="Type of data to import",
    )
    parser.add_argument(
        "--uri",
        type=str,
        default=os.getenv("NEO4J_URI", "bolt://localhost:7687"),
        help="Neo4j bolt URI",
    )
    parser.add_argument(
        "--user",
        type=str,
        default=os.getenv("NEO4J_USER", "neo4j"),
        help="Neo4j user",
    )
    parser.add_argument(
        "--password",
        type=str,
        default=os.getenv("NEO4J_PASSWORD", "password"),
        help="Neo4j password",
    )
    parser.add_argument(
        "--batch-size",
        type=int,
        default=500,
        help="Number of items per transaction",
    )
    parser.add_argument(
        "--limit",
        type=int,
        default=None,
        help="Import only the first N items",
    )
    parser.add_argument(
        "--dry-run",
        action="store_true",
        help="Preview import without writing to Neo4j",
    )
    parser.add_argument(
        "--reset",
        action="store_true",
        help="Delete existing Work nodes before importing",
    )
    parser.add_argument(
        "--no-relationships",
        action="store_true",
        help="Skip creating Author/Magazine/Studio relationships",
    )
    parser.add_argument(
        "--publisher-mapping",
        type=Path,
        default=DEFAULT_PUBLISHER_MAPPING,
        help="Path to publisher_magazine_mapping.json for PUBLISHED_BY relationships",
    )
    
    args = parser.parse_args(argv)
    
    publisher_path = args.publisher_mapping.expanduser().resolve()
    if not publisher_path.exists():
        publisher_path = None
    
    return ImportConfig(
        input_dir=args.input_dir.expanduser().resolve(),
        import_type=args.type,
        uri=args.uri,
        user=args.user,
        password=args.password,
        batch_size=args.batch_size,
        limit=args.limit,
        dry_run=args.dry_run,
        reset=args.reset,
        create_relationships=not args.no_relationships,
        publisher_mapping_path=publisher_path,
    )


def chunked(iterable: Iterable[Any], size: int) -> Iterator[List[Any]]:
    """Yield chunks of the given size from the iterable."""
    chunk: List[Any] = []
    for item in iterable:
        chunk.append(item)
        if len(chunk) >= size:
            yield chunk
            chunk = []
    if chunk:
        yield chunk


def load_json_data(input_dir: Path, data_type: str) -> List[Dict[str, Any]]:
    """Load JSON data from input directory."""
    all_data: List[Dict[str, Any]] = []
    
    # Find all matching JSON files
    pattern = f"{data_type}_*.json"
    files = list(input_dir.glob(pattern))
    
    if not files:
        print(f"No {data_type} JSON files found in {input_dir}")
        return []
    
    for file_path in files:
        print(f"Loading {file_path}...")
        with open(file_path, "r", encoding="utf-8") as f:
            data = json.load(f)
            
            # Handle both formats: {metadata, manga/anime} or just a list
            if isinstance(data, dict):
                items = data.get(data_type, data.get("manga", data.get("anime", [])))
            else:
                items = data
            
            all_data.extend(items)
    
    # Deduplicate by ID
    seen_ids: Set[str] = set()
    unique_data: List[Dict[str, Any]] = []
    for item in all_data:
        item_id = str(item.get("id", item.get("mal_id", "")))
        if item_id and item_id not in seen_ids:
            seen_ids.add(item_id)
            unique_data.append(item)
    
    print(f"Loaded {len(unique_data)} unique {data_type} entries")
    return unique_data


def prepare_work_properties(item: Dict[str, Any]) -> Dict[str, Any]:
    """Prepare properties for Work node creation.
    
    Removes nested objects that need to be stored as relationships.
    """
    # Properties that should be stored as relationships, not on the node
    relationship_fields = {
        "author_roles",
        "related_manga",
        "related_anime", 
        "recommendations",
    }
    
    # Properties that need special handling
    list_to_json_fields = {
        "pictures",  # Store as JSON string
    }
    
    properties = {}
    for key, value in item.items():
        if key in relationship_fields:
            continue
        
        if value is None:
            continue
            
        if key in list_to_json_fields and isinstance(value, list):
            properties[key] = json.dumps(value)
        else:
            properties[key] = value
    
    return properties


class MalApiImporter:
    """Importer for MAL API JSON data into Neo4j."""
    
    def __init__(self, driver: Driver, batch_size: int = 500) -> None:
        self.driver = driver
        self.batch_size = batch_size
    
    def ensure_constraints(self) -> None:
        """Create necessary constraints and indexes."""
        constraints = [
            "CREATE CONSTRAINT IF NOT EXISTS FOR (w:Work) REQUIRE w.id IS UNIQUE",
            "CREATE CONSTRAINT IF NOT EXISTS FOR (a:Author) REQUIRE a.name IS UNIQUE",
            "CREATE CONSTRAINT IF NOT EXISTS FOR (m:Magazine) REQUIRE m.name IS UNIQUE",
            "CREATE CONSTRAINT IF NOT EXISTS FOR (s:Studio) REQUIRE s.name IS UNIQUE",
            "CREATE CONSTRAINT IF NOT EXISTS FOR (p:Publisher) REQUIRE p.name IS UNIQUE",
        ]
        
        with self.driver.session() as session:
            for constraint in constraints:
                try:
                    session.run(constraint)
                except Exception as e:
                    print(f"Warning: {e}")
    
    def clear_nodes(self, label: str = "Work") -> None:
        """Delete all nodes with the given label."""
        with self.driver.session() as session:
            session.run(f"MATCH (n:{label}) DETACH DELETE n")
    
    def import_works(self, works: List[Dict[str, Any]]) -> None:
        """Import Work nodes in batches."""
        total_batches = math.ceil(len(works) / self.batch_size)
        
        for chunk in tqdm(
            chunked(works, self.batch_size),
            total=total_batches,
            desc="Importing Work nodes",
        ):
            self._import_work_chunk(chunk)
    
    def _import_work_chunk(self, chunk: List[Dict[str, Any]]) -> None:
        """Import a chunk of Work nodes."""
        # Prepare data for import
        rows = []
        for item in chunk:
            properties = prepare_work_properties(item)
            rows.append({
                "id": str(item.get("id")),
                "properties": properties,
            })
        
        with self.driver.session() as session:
            session.execute_write(self._write_work_chunk, rows)
    
    @staticmethod
    def _write_work_chunk(tx, rows: List[Dict[str, Any]]) -> None:
        tx.run(
            """
            UNWIND $rows AS row
            MERGE (w:Work {id: row.id})
            SET w += row.properties
            """,
            rows=rows,
        )
    
    def import_authors(self, works: List[Dict[str, Any]]) -> None:
        """Create Author nodes and CREATED_BY relationships."""
        # Collect all unique authors
        authors: Dict[str, Set[str]] = {}  # author_name -> set of work_ids
        
        for work in works:
            work_id = str(work.get("id"))
            author_roles = work.get("author_roles", [])
            
            # Handle both formats: list of dicts or list of strings
            if author_roles and isinstance(author_roles[0], dict):
                for ar in author_roles:
                    name = ar.get("name", "").strip()
                    if name:
                        if name not in authors:
                            authors[name] = set()
                        authors[name].add(work_id)
            else:
                # Fallback to 'authors' field
                for name in work.get("authors", []):
                    name = str(name).strip()
                    if name:
                        if name not in authors:
                            authors[name] = set()
                        authors[name].add(work_id)
        
        if not authors:
            print("No authors found")
            return
        
        print(f"Creating {len(authors)} Author nodes...")
        
        # Create Author nodes
        author_rows = [{"name": name} for name in authors.keys()]
        for chunk in chunked(author_rows, self.batch_size):
            with self.driver.session() as session:
                session.execute_write(self._create_authors, chunk)
        
        # Create relationships
        print("Creating CREATED_BY relationships...")
        rel_rows = []
        for author_name, work_ids in authors.items():
            for work_id in work_ids:
                rel_rows.append({"author_name": author_name, "work_id": work_id})
        
        for chunk in tqdm(chunked(rel_rows, self.batch_size), 
                         total=math.ceil(len(rel_rows) / self.batch_size),
                         desc="Creating CREATED_BY"):
            with self.driver.session() as session:
                session.execute_write(self._create_author_relationships, chunk)
    
    @staticmethod
    def _create_authors(tx, rows: List[Dict[str, Any]]) -> None:
        tx.run(
            """
            UNWIND $rows AS row
            MERGE (a:Author {name: row.name})
            """,
            rows=rows,
        )
    
    @staticmethod
    def _create_author_relationships(tx, rows: List[Dict[str, Any]]) -> None:
        tx.run(
            """
            UNWIND $rows AS row
            MATCH (w:Work {id: row.work_id})
            MATCH (a:Author {name: row.author_name})
            MERGE (w)-[:CREATED_BY]->(a)
            """,
            rows=rows,
        )
    
    def import_magazines(self, works: List[Dict[str, Any]]) -> None:
        """Create Magazine nodes and PUBLISHED_IN relationships."""
        magazines: Dict[str, Set[str]] = {}  # magazine_name -> set of work_ids
        
        for work in works:
            work_id = str(work.get("id"))
            serialization = work.get("serialization", [])
            
            for mag_name in serialization:
                mag_name = str(mag_name).strip()
                if mag_name:
                    if mag_name not in magazines:
                        magazines[mag_name] = set()
                    magazines[mag_name].add(work_id)
        
        if not magazines:
            print("No magazines found")
            return
        
        print(f"Creating {len(magazines)} Magazine nodes...")
        
        # Create Magazine nodes
        mag_rows = [{"name": name} for name in magazines.keys()]
        for chunk in chunked(mag_rows, self.batch_size):
            with self.driver.session() as session:
                session.execute_write(self._create_magazines, chunk)
        
        # Create relationships
        print("Creating PUBLISHED_IN relationships...")
        rel_rows = []
        for mag_name, work_ids in magazines.items():
            for work_id in work_ids:
                rel_rows.append({"magazine_name": mag_name, "work_id": work_id})
        
        for chunk in tqdm(chunked(rel_rows, self.batch_size),
                         total=math.ceil(len(rel_rows) / self.batch_size),
                         desc="Creating PUBLISHED_IN"):
            with self.driver.session() as session:
                session.execute_write(self._create_magazine_relationships, chunk)
    
    @staticmethod
    def _create_magazines(tx, rows: List[Dict[str, Any]]) -> None:
        tx.run(
            """
            UNWIND $rows AS row
            MERGE (m:Magazine {name: row.name})
            """,
            rows=rows,
        )
    
    @staticmethod
    def _create_magazine_relationships(tx, rows: List[Dict[str, Any]]) -> None:
        tx.run(
            """
            UNWIND $rows AS row
            MATCH (w:Work {id: row.work_id})
            MATCH (m:Magazine {name: row.magazine_name})
            MERGE (w)-[:PUBLISHED_IN]->(m)
            """,
            rows=rows,
        )
    
    def import_studios(self, works: List[Dict[str, Any]]) -> None:
        """Create Studio nodes and PRODUCED_BY relationships (for anime)."""
        studios: Dict[str, Set[str]] = {}  # studio_name -> set of work_ids
        
        for work in works:
            work_id = str(work.get("id"))
            studio_list = work.get("studios", [])
            
            for studio_name in studio_list:
                studio_name = str(studio_name).strip()
                if studio_name:
                    if studio_name not in studios:
                        studios[studio_name] = set()
                    studios[studio_name].add(work_id)
        
        if not studios:
            print("No studios found")
            return
        
        print(f"Creating {len(studios)} Studio nodes...")
        
        # Create Studio nodes
        studio_rows = [{"name": name} for name in studios.keys()]
        for chunk in chunked(studio_rows, self.batch_size):
            with self.driver.session() as session:
                session.execute_write(self._create_studios, chunk)
        
        # Create relationships
        print("Creating PRODUCED_BY relationships...")
        rel_rows = []
        for studio_name, work_ids in studios.items():
            for work_id in work_ids:
                rel_rows.append({"studio_name": studio_name, "work_id": work_id})
        
        for chunk in tqdm(chunked(rel_rows, self.batch_size),
                         total=math.ceil(len(rel_rows) / self.batch_size),
                         desc="Creating PRODUCED_BY"):
            with self.driver.session() as session:
                session.execute_write(self._create_studio_relationships, chunk)
    
    @staticmethod
    def _create_studios(tx, rows: List[Dict[str, Any]]) -> None:
        tx.run(
            """
            UNWIND $rows AS row
            MERGE (s:Studio {name: row.name})
            """,
            rows=rows,
        )
    
    @staticmethod
    def _create_studio_relationships(tx, rows: List[Dict[str, Any]]) -> None:
        tx.run(
            """
            UNWIND $rows AS row
            MATCH (w:Work {id: row.work_id})
            MATCH (s:Studio {name: row.studio_name})
            MERGE (w)-[:PRODUCED_BY]->(s)
            """,
            rows=rows,
        )
    
    def import_related_works(self, works: List[Dict[str, Any]]) -> None:
        """Create RELATED_TO relationships between works."""
        relationships: List[Dict[str, Any]] = []
        
        for work in works:
            work_id = str(work.get("id"))
            
            # Related manga
            for rm in work.get("related_manga", []):
                related_id = rm.get("id")
                relation_type = rm.get("relation_type", "related")
                if related_id:
                    relationships.append({
                        "from_id": work_id,
                        "to_id": str(related_id),
                        "relation_type": relation_type,
                    })
            
            # Related anime
            for ra in work.get("related_anime", []):
                related_id = ra.get("id")
                relation_type = ra.get("relation_type", "related")
                if related_id:
                    relationships.append({
                        "from_id": work_id,
                        "to_id": f"anime_{related_id}",  # Anime IDs are prefixed
                        "relation_type": relation_type,
                    })
        
        if not relationships:
            print("No related work relationships found")
            return
        
        print(f"Creating {len(relationships)} RELATED_TO relationships...")
        
        for chunk in tqdm(chunked(relationships, self.batch_size),
                         total=math.ceil(len(relationships) / self.batch_size),
                         desc="Creating RELATED_TO"):
            with self.driver.session() as session:
                session.execute_write(self._create_related_relationships, chunk)
    
    @staticmethod
    def _create_related_relationships(tx, rows: List[Dict[str, Any]]) -> None:
        # Only create relationship if both nodes exist
        tx.run(
            """
            UNWIND $rows AS row
            MATCH (w1:Work {id: row.from_id})
            MATCH (w2:Work {id: row.to_id})
            MERGE (w1)-[r:RELATED_TO]->(w2)
            SET r.relation_type = row.relation_type
            """,
            rows=rows,
        )

    def import_publishers(self, mapping_path: Path) -> None:
        """Create Publisher nodes and PUBLISHED_BY relationships from mapping file."""
        if not mapping_path or not mapping_path.exists():
            print(f"Publisher mapping file not found: {mapping_path}")
            return
        
        print(f"Loading publisher mapping from {mapping_path}...")
        with open(mapping_path, "r", encoding="utf-8") as f:
            mapping = json.load(f)
        
        if not mapping:
            print("No publisher mappings found")
            return
        
        print(f"Creating {len(mapping)} Publisher nodes and PUBLISHED_BY relationships...")
        
        # Convert to list of entries for batch processing
        entries = [
            {"publisher": publisher, "magazines": magazines}
            for publisher, magazines in mapping.items()
            if publisher and magazines
        ]
        
        for chunk in tqdm(
            chunked(entries, self.batch_size),
            total=math.ceil(len(entries) / self.batch_size),
            desc="Creating PUBLISHED_BY"
        ):
            with self.driver.session() as session:
                session.execute_write(self._create_publisher_relationships, chunk)
    
    @staticmethod
    def _create_publisher_relationships(tx, rows: List[Dict[str, Any]]) -> None:
        tx.run(
            """
            UNWIND $rows AS row
            MERGE (p:Publisher {name: row.publisher})
            FOREACH (magazine IN row.magazines |
                MERGE (m:Magazine {name: magazine})
                MERGE (m)-[:PUBLISHED_BY]->(p)
            )
            """,
            rows=rows,
        )


def main(argv: Optional[List[str]] = None) -> int:
    config = parse_args(argv)
    
    # Load data
    all_works: List[Dict[str, Any]] = []
    
    if config.import_type in ("manga", "both"):
        manga_data = load_json_data(config.input_dir, "manga")
        all_works.extend(manga_data)
    
    if config.import_type in ("anime", "both"):
        anime_data = load_json_data(config.input_dir, "anime")
        all_works.extend(anime_data)
    
    if not all_works:
        print("No data to import!")
        return 1
    
    if config.limit:
        all_works = all_works[:config.limit]
    
    print(f"Total items to import: {len(all_works)}")
    
    if config.dry_run:
        print("\nDry-run preview (first 3 items):")
        for idx, work in enumerate(all_works[:3], start=1):
            print(f"  {idx}. id={work.get('id')}, title={work.get('title_name')}")
            print(f"      genres={work.get('genres', [])[:3]}")
            print(f"      authors={work.get('authors', [])[:2]}")
        return 0
    
    # Connect to Neo4j
    driver = GraphDatabase.driver(config.uri, auth=(config.user, config.password))
    importer = MalApiImporter(driver, batch_size=config.batch_size)
    
    try:
        # Setup constraints
        print("Creating constraints...")
        importer.ensure_constraints()
        
        # Reset if requested
        if config.reset:
            print("Resetting database (deleting existing Work nodes)...")
            importer.clear_nodes("Work")
        
        # Import Work nodes
        importer.import_works(all_works)
        
        # Create relationships if enabled
        if config.create_relationships:
            # Filter manga and anime for relationship creation
            manga_works = [w for w in all_works if not str(w.get("id", "")).startswith("anime_")]
            anime_works = [w for w in all_works if str(w.get("id", "")).startswith("anime_")]
            
            if manga_works:
                importer.import_authors(manga_works)
                importer.import_magazines(manga_works)
            
            if anime_works:
                importer.import_studios(anime_works)
            
            # Related works (applies to both)
            importer.import_related_works(all_works)
            
            # Publisher -> Magazine relationships (PUBLISHED_BY)
            if config.publisher_mapping_path:
                importer.import_publishers(config.publisher_mapping_path)
        
        print("\nImport completed successfully!")
        
    finally:
        driver.close()
    
    return 0


if __name__ == "__main__":
    sys.exit(main())
